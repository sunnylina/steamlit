import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import KNNImputer
import torch
import torch.optim as optim
import os
import time
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(
    page_title="ğŸ§Š IP ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS ì ìš©
st.markdown("""
    <style>
        .stApp {
            background-color: #FFF0F5;
        }
        .stButton>button {
            background-color: #FF69B4;
            color: white;
        }
        .stSelectbox {
            background-color: #FFE4E1;
        }
        .css-1d391kg {
            background-color: #FFB6C1;
        }
        .sidebar .sidebar-content {
            background-color: #FFC0CB;
        }
    </style>
""", unsafe_allow_html=True)


# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess_edr_data(df, user_col, proc_col, time_col, bytes_col, impute_method='knn'):
    df_processed = df.copy()

    # ë ˆì´ë¸” ì¸ì½”ë”© - ì‚¬ìš©ì ë° í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ«ìë¡œ ë³€í™˜
    user_encoder = LabelEncoder()
    proc_encoder = LabelEncoder()

    df_processed[user_col] = user_encoder.fit_transform(df_processed[user_col])
    df_processed[proc_col] = proc_encoder.fit_transform(df_processed[proc_col])

    user_mapping = dict(zip(user_encoder.classes_, range(len(user_encoder.classes_))))
    proc_mapping = dict(zip(proc_encoder.classes_, range(len(proc_encoder.classes_))))

    # ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
    df_processed[time_col] = pd.to_datetime(df_processed[time_col])
    df_processed[time_col] = (df_processed[time_col] - df_processed[time_col].min()).dt.total_seconds()

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ëŠ” ìœ ì§€ (í•„ìš”í•œ ê²½ìš°)
    if impute_method == 'knn':
        imputer = KNNImputer(n_neighbors=5)
        df_processed[bytes_col] = imputer.fit_transform(df_processed[[bytes_col]])

    # ì •ê·œí™” ì½”ë“œ ì œê±° - bytes_colì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©

    return df_processed, user_mapping, proc_mapping, user_encoder, proc_encoder, None  # scalerëŠ” Noneìœ¼ë¡œ ë°˜í™˜


class EDR_OnlineDMHP(torch.nn.Module):
    def __init__(self, num_users, num_processes, **params):
        super(EDR_OnlineDMHP, self).__init__()
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        self.default_params = {
            'feature_dim': 1,
            'hidden_dim': 256,
            'lr': 0.0001,
            'time_window': 48,
            'max_history_length': 5000,
            'warm_up_period': 2000,
            'update_interval': 50,
            'checkpoint_interval': 5000,
            'threshold': 3.0,
            'decay_rate': 0.05,
            'min_events_for_anomaly': 10
        }
        # ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°ë¡œ ê¸°ë³¸ê°’ ì—…ë°ì´íŠ¸
        self.params = {**self.default_params, **params}

        self.num_users = num_users
        self.num_processes = num_processes
        self.feature_dim = self.params['feature_dim']
        self.hidden_dim = self.params['hidden_dim']

        # ì„ë² ë”© ë ˆì´ì–´
        self.user_embedding = torch.nn.Embedding(num_users, self.hidden_dim * 2)
        self.process_embedding = torch.nn.Embedding(num_processes, self.hidden_dim * 2)

        # BatchNorm1dë¥¼ ì œê±°í•˜ê³  Layer Normalizationìœ¼ë¡œ ëŒ€ì²´
        self.fc1 = torch.nn.Linear(self.hidden_dim * 4 + self.feature_dim, self.hidden_dim * 2)
        self.ln1 = torch.nn.LayerNorm(self.hidden_dim * 2)
        self.dropout1 = torch.nn.Dropout(0.3)

        self.fc2 = torch.nn.Linear(self.hidden_dim * 2, self.hidden_dim)
        self.ln2 = torch.nn.LayerNorm(self.hidden_dim)
        self.dropout2 = torch.nn.Dropout(0.2)

        self.fc3 = torch.nn.Linear(self.hidden_dim, self.hidden_dim // 2)
        self.ln3 = torch.nn.LayerNorm(self.hidden_dim // 2)

        self.fc4 = torch.nn.Linear(self.hidden_dim // 2, 1)

    def forward(self, user_ids, proc_ids, features):
        user_emb = self.user_embedding(user_ids)
        process_emb = self.process_embedding(proc_ids)
        combined = torch.cat([user_emb, process_emb, features], dim=1)

        x = self.fc1(combined)
        x = self.ln1(x)  # BatchNormì„ LayerNormìœ¼ë¡œ ë³€ê²½
        x = torch.relu(x)
        x = self.dropout1(x)

        x = self.fc2(x)
        x = self.ln2(x)  # BatchNormì„ LayerNormìœ¼ë¡œ ë³€ê²½
        x = torch.relu(x)
        x = self.dropout2(x)

        x = self.fc3(x)
        x = self.ln3(x)  # BatchNormì„ LayerNormìœ¼ë¡œ ë³€ê²½
        x = torch.relu(x)

        intensity = torch.exp(self.fc4(x))

        return intensity


# edr_online_train_and_detect í•¨ìˆ˜ ìˆ˜ì •
def edr_online_train_and_detect(df, num_users, num_processes, user_encoder, proc_encoder, original_df, **params):
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
    default_params = {
        'feature_dim': 1,
        'hidden_dim': 256,
        'lr': 0.0001,
        'time_window': 48,
        'max_history_length': 5000,
        'warm_up_period': 2000,
        'update_interval': 50,
        'checkpoint_interval': 5000,
        'threshold': 60.0,  # ì„ê³„ê°’ ê¸°ë³¸ê°’ ìˆ˜ì • (0~100 ê¸°ì¤€)
        'decay_rate': 0.05,
        'min_events_for_anomaly': 10,
        'user_col': 'user_id',
        'proc_col': 'process_id',
        'time_col': 'timestamp',
        'bytes_col': 'bytes',
        'score_scale': 1.5,  # ì‹œê·¸ëª¨ì´ë“œ ê¸°ìš¸ê¸° ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ë” ê°€íŒŒë¥¸ ê¸°ìš¸ê¸°)
        'score_max': 100.0  # ì›ë³¸ ì ìˆ˜ì˜ ìµœëŒ€ ì œí•œê°’
    }

    # ì‚¬ìš©ì ì§€ì • íŒŒë¼ë¯¸í„°ë¡œ ê¸°ë³¸ê°’ ì—…ë°ì´íŠ¸
    params = {**default_params, **params}

    # ëª¨ë“  ì‚¬ìš©ì(IP)ë¥¼ ê°€ì ¸ì˜¤ê¸°
    unique_users = df[params['user_col']].unique()

    # IPë³„ ëª¨ë¸, ìµœì í™”ê¸°, ì´ë ¥ ë° ì†ì‹¤ ê´€ë¦¬ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    models = {}
    optimizers = {}
    histories = {}
    all_losses = {}

    # ì´ìƒì¹˜ ì ìˆ˜ ì •ê·œí™”ë¥¼ ìœ„í•œ ì´ë™ ìœˆë„ìš° ì‚¬ìš©
    recent_scores = []  # ìµœê·¼ ì›ë³¸ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    score_window_size = 1000  # ì ìˆ˜ ì •ê·œí™”ì— ì‚¬ìš©í•  ìœˆë„ìš° í¬ê¸°

    # ê° IPë³„ë¡œ ë³„ë„ì˜ ëª¨ë¸ ë° ê´€ë ¨ ê°ì²´ ì´ˆê¸°í™”
    for user_id in unique_users:
        models[user_id] = EDR_OnlineDMHP(num_users, num_processes, **params)
        optimizers[user_id] = optim.Adam(models[user_id].parameters(), lr=params['lr'])
        histories[user_id] = []
        all_losses[user_id] = []

    # ì „ì²´ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    results = []

    # ê° IPë³„ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸ (ì›œì—… í™•ì¸ìš©)
    event_counts = {user_id: 0 for user_id in unique_users}

    # ì§„í–‰ ìƒíƒœ í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    loss_text = st.empty()

    start_time = time.time()
    initial_time = pd.Timestamp(df[params['time_col']].iloc[0])

    for i in range(len(df)):
        if i % 100 == 0:
            current_time = time.time()
            progress = (i + 1) / len(df)
            progress_bar.progress(progress)

            elapsed_time = current_time - start_time
            time_per_item = elapsed_time / (i + 1) if i > 0 else 0
            remaining_items = len(df) - (i + 1)
            estimated_remaining = time_per_item * remaining_items

            status_text.text(
                f"ì²˜ë¦¬ ì¤‘... {i + 1:,}/{len(df):,} ({progress:.1%})\n"
                f"ê²½ê³¼ ì‹œê°„: {elapsed_time / 60:.1f}ë¶„ | "
                f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining / 60:.1f}ë¶„"
            )

        current_data = df.iloc[i]
        original_data = original_df.iloc[i]  # ì›ë³¸ ë°ì´í„°(ì¸ì½”ë”© ì „)ì— ì ‘ê·¼

        # í˜„ì¬ ë°ì´í„°ì˜ ì‚¬ìš©ì ID
        user_id = current_data[params['user_col']]
        user_id_tensor = torch.tensor([user_id], dtype=torch.long)
        proc_id = torch.tensor([current_data[params['proc_col']]], dtype=torch.long)
        feature = torch.tensor([[current_data[params['bytes_col']]]], dtype=torch.float)

        # í˜„ì¬ ì‚¬ìš©ìì˜ ì´ë²¤íŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
        event_counts[user_id] += 1

        event_time = pd.Timestamp(current_data[params['time_col']])
        current_time = (event_time - initial_time).total_seconds() / 3600

        # í˜„ì¬ ì‚¬ìš©ìì˜ ì´ë ¥ ê´€ë¦¬
        history = histories[user_id]
        history = [h for h in history if (current_time - h['time']) <= params['time_window']]

        if len(history) > 0:
            historical_influence = 0
            for h in history:
                time_diff = current_time - h['time']
                decay = np.exp(-params['decay_rate'] * time_diff)
                historical_influence += decay
        else:
            historical_influence = 0

        # í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë¸ì— ëŒ€í•œ ì˜ˆì¸¡
        model = models[user_id]
        intensity = model(user_id_tensor, proc_id, feature)

        # IPë³„ ì›œì—… ê¸°ê°„ í™•ì¸ í›„ ì´ìƒì¹˜ íƒì§€ (ì›œì—… ê¸°ê°„ ì´í›„ì—ë§Œ)
        if event_counts[user_id] > params['warm_up_period']:
            proc_history = [h for h in history if h['proc_id'].item() == proc_id.item()]

            if len(history) >= params['min_events_for_anomaly']:
                # intensity ê°’ì´ ë„ˆë¬´ ì‘ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í´ë¦¬í•‘
                clipped_intensity = torch.clamp(intensity, min=1e-10)
                base_score = torch.abs(1.0 / clipped_intensity).item()  # ê°•ë„ì˜ ì—­ìˆ˜ë¥¼ ì‚¬ìš©

                # ì‚¬ìš©ì ë° í”„ë¡œì„¸ìŠ¤ë³„ í†µê³„ ê³„ì‚°
                user_features = [h['feature'].item() for h in history]
                user_mean = np.mean(user_features)
                user_std = np.std(user_features) + 1e-6

                proc_features = [h['feature'].item() for h in proc_history] if proc_history else user_features
                proc_mean = np.mean(proc_features)
                proc_std = np.std(proc_features) + 1e-6

                # ì •ê·œí™”ëœ íŠ¹ì„±ê°’ ê³„ì‚°
                current_feature = feature.item()
                normalized_feature = abs(current_feature - user_mean) / user_std
                proc_normalized = abs(current_feature - proc_mean) / proc_std

                # ê¸°ë³¸ ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚° - ê°€ì¤‘ì¹˜ ì¡°ì •ìœ¼ë¡œ ì ìˆ˜ ë²”ìœ„ í™•ëŒ€
                raw_score = base_score * \
                            (1 + np.log1p(historical_influence)) * \
                            (1 + np.sqrt(normalized_feature)) * \
                            (1 + np.sqrt(proc_normalized))

                # ì ìˆ˜ í´ë¦¬í•‘ - ë„ˆë¬´ í° ê°’ ë°©ì§€
                raw_score = min(raw_score, params['score_max'])

                # ìµœê·¼ ì ìˆ˜ ìœˆë„ìš°ì— ì¶”ê°€
                recent_scores.append(raw_score)
                if len(recent_scores) > score_window_size:
                    recent_scores.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ì ìˆ˜ ì œê±°

                # ì ìˆ˜ ì •ê·œí™” ë° ë³€í™˜ - ë” ë„“ì€ ë¶„í¬ ìƒì„±
                if len(recent_scores) > 100:
                    # í˜„ì¬ ì ìˆ˜ì˜ ë°±ë¶„ìœ„ ê³„ì‚° (0~1)
                    percentile = sum(1 for s in recent_scores if s <= raw_score) / len(recent_scores)

                    # ë°±ë¶„ìœ„ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ (0-100)
                    if percentile > 0.99:
                        # ìƒìœ„ 1%ëŠ” 90-100ì  ë²”ìœ„
                        anomaly_score = 90 + (percentile - 0.99) * 1000
                    elif percentile > 0.95:
                        # ìƒìœ„ 1-5%ëŠ” 80-90ì  ë²”ìœ„
                        anomaly_score = 80 + (percentile - 0.95) * 250
                    elif percentile > 0.9:
                        # ìƒìœ„ 5-10%ëŠ” 70-80ì  ë²”ìœ„
                        anomaly_score = 70 + (percentile - 0.9) * 200
                    elif percentile > 0.8:
                        # ìƒìœ„ 10-20%ëŠ” 60-70ì  ë²”ìœ„
                        anomaly_score = 60 + (percentile - 0.8) * 100
                    elif percentile > 0.7:
                        # ìƒìœ„ 20-30%ëŠ” 50-60ì  ë²”ìœ„
                        anomaly_score = 50 + (percentile - 0.7) * 100
                    elif percentile > 0.5:
                        # ìƒìœ„ 30-50%ëŠ” 30-50ì  ë²”ìœ„
                        anomaly_score = 30 + (percentile - 0.5) * 100
                    else:
                        # í•˜ìœ„ 50%ëŠ” 0-30ì  ë²”ìœ„
                        anomaly_score = percentile * 60
                else:
                    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìŒ“ì´ê¸° ì „ì—ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ë³€í™˜
                    sigmoid_score = 1.0 / (1.0 + np.exp(-raw_score / params['score_scale']))
                    anomaly_score = sigmoid_score * 100.0

                # ìµœì¢… ì ìˆ˜ ì œí•œ (0-100)
                anomaly_score = max(0, min(anomaly_score, 100.0))

                # ì´ìƒì¹˜ íŒë‹¨ (ì •ê·œí™”ëœ ì ìˆ˜ ê¸°ì¤€)
                is_anomaly = anomaly_score > params['threshold']

                # ì¸ì½”ë”©ëœ ê°’ ëŒ€ì‹  ì›ë³¸ ê°’ ì‚¬ìš©
                results.append({
                    'time': original_data[params['time_col']],
                    'user': original_data[params['user_col']],  # ì›ë³¸ IP ì£¼ì†Œ
                    'process': original_data[params['proc_col']],  # ì›ë³¸ í”„ë¡œì„¸ìŠ¤ ì´ë¦„
                    'bytes': original_data[params['bytes_col']],
                    'raw_score': raw_score,  # ì •ê·œí™” ì „ ì›ë³¸ ì ìˆ˜
                    'anomaly_score': anomaly_score,  # ì •ê·œí™”ëœ ìµœì¢… ì ìˆ˜ (0~100)
                    'historical_influence': historical_influence,
                    'normalized_feature': normalized_feature,
                    'proc_normalized': proc_normalized,
                    'intensity': intensity.item(),
                    'is_anomaly': is_anomaly
                })

        # ëª¨ë¸ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë¸ë§Œ ì—…ë°ì´íŠ¸)
        if i % params['update_interval'] == 0:
            optimizer = optimizers[user_id]
            optimizer.zero_grad()
            loss = -torch.log(intensity).mean() * (1 + historical_influence)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            # í˜„ì¬ ì‚¬ìš©ìì˜ ì†ì‹¤ ê¸°ë¡
            all_losses[user_id].append(loss.item())

            # ëª¨ë“  ì‚¬ìš©ìì˜ í‰ê·  ì†ì‹¤ í‘œì‹œ
            all_recent_losses = []
            for u_id in unique_users:
                if all_losses[u_id]:  # ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ
                    all_recent_losses.extend(all_losses[u_id][-5:])

            if all_recent_losses:
                avg_loss = sum(all_recent_losses) / len(all_recent_losses)
                loss_text.text(f"í˜„ì¬ Loss: {loss.item():.4f} | ì „ì²´ í‰ê·  Loss: {avg_loss:.4f}")

        # í˜„ì¬ ì‚¬ìš©ìì˜ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        histories[user_id].append({
            'time': current_time,
            'user_id': user_id_tensor,
            'proc_id': proc_id,
            'feature': feature
        })

        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ
        if len(histories[user_id]) > params['max_history_length']:
            histories[user_id] = histories[user_id][-params['max_history_length']:]

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë“  IPì˜ ëª¨ë¸ì„ í•¨ê»˜ ì €ì¥)
        if i % params['checkpoint_interval'] == 0 and i > 0:
            checkpoint_path = f"checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pt"
            save_dict = {
                'iteration': i,
                'histories': histories,
                'all_losses': all_losses,
                'recent_scores': recent_scores  # ì ìˆ˜ ì •ê·œí™”ì— í•„ìš”í•œ ìµœê·¼ ì´ìƒì¹˜ ì ìˆ˜ë„ ì €ì¥
            }

            # ê° IPë³„ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ì €ì¥
            for u_id in unique_users:
                save_dict[f'model_state_{u_id}'] = models[u_id].state_dict()
                save_dict[f'optimizer_state_{u_id}'] = optimizers[u_id].state_dict()

            torch.save(save_dict, checkpoint_path)

    progress_bar.empty()
    status_text.empty()
    loss_text.empty()

    # ëª¨ë“  IPì˜ ì†ì‹¤ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°í•© (ì‹œê°í™”ìš©)
    combined_losses = []
    for u_id in unique_users:
        combined_losses.extend(all_losses[u_id])

    # ì²« ë²ˆì§¸ ëª¨ë¸ì„ ë°˜í™˜ (í˜¸í™˜ì„± ìœ„í•´)
    return models[unique_users[0]], pd.DataFrame(results), histories, combined_losses


# ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™” í•¨ìˆ˜
def analyze_and_visualize_results(results_df, losses):
    # ê²°ê³¼ ë¶„ì„
    total_events = len(results_df)
    total_anomalies = results_df['is_anomaly'].sum()
    anomaly_rate = (total_anomalies / total_events) * 100

    # ì‹œê°í™”
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### ğŸ“Š ì´ìƒì¹˜ íƒì§€ ê²°ê³¼")
        st.metric("ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜", f"{total_events:,}")
        st.metric("íƒì§€ëœ ì´ìƒì¹˜ ìˆ˜", f"{int(total_anomalies):,}")
        st.metric("ì´ìƒì¹˜ ë¹„ìœ¨", f"{anomaly_rate:.2f}%")

        # ì‹œê³„ì—´ ê·¸ë˜í”„
        fig_timeline = px.scatter(results_df,
                                  x='time',
                                  y='anomaly_score',
                                  color='is_anomaly',
                                  title='ğŸ•’ ì‹œê°„ë³„ ì´ìƒì¹˜ ì ìˆ˜')
        st.plotly_chart(fig_timeline, use_container_width=True)

    with col2:
        # ì‚¬ìš©ìë³„ ì´ìƒì¹˜ ë¶„í¬
        user_anomalies = results_df[results_df['is_anomaly']]['user'].value_counts()
        fig_users = px.bar(user_anomalies,
                           title='ğŸ‘¤ ì‚¬ìš©ìë³„ ì´ìƒì¹˜ ë°œìƒ íšŸìˆ˜',
                           labels={'index': 'ì‚¬ìš©ì', 'value': 'ì´ìƒì¹˜ íšŸìˆ˜'})
        st.plotly_chart(fig_users, use_container_width=True)

        # í•™ìŠµ ì†ì‹¤ ê·¸ë˜í”„
        fig_loss = px.line(y=losses,
                           title='ğŸ“‰ í•™ìŠµ ì†ì‹¤ ì¶”ì´',
                           labels={'index': 'ë°˜ë³µ íšŸìˆ˜', 'value': 'ì†ì‹¤ê°’'})
        st.plotly_chart(fig_loss, use_container_width=True)

    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
    with st.expander("ğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ë³´ê¸°"):
        st.dataframe(results_df[results_df['is_anomaly']].sort_values('anomaly_score', ascending=False))


def main():
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("âœ¨ ì„¤ì • ë©”ë‰´")

    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.sidebar.markdown("### ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”! ğŸ˜Š", type=['csv'])

    # ë©”ì¸ í˜ì´ì§€ ì œëª©
    st.title("ğŸ’ IP ê¸°ë°˜ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ")
    st.markdown("#### ğŸŒŸ ë§ˆí¬ëœ Hawkes Processë¥¼ ì‚¬ìš©í•œ IPë³„ ì´ìƒì¹˜ íƒì§€")

    if uploaded_file is not None:
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(uploaded_file)

        # ì‚¬ì´ë“œë°”ì— íŒŒë¼ë¯¸í„° ì„¤ì •
        st.sidebar.markdown("### âš™ï¸ ì»¬ëŸ¼ ì„¤ì •")
        time_col = st.sidebar.selectbox("ì‹œê°„ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš” â°", df.columns)
        user_col = st.sidebar.selectbox("ì‚¬ìš©ì ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš” ğŸ‘¤", df.columns)
        proc_col = st.sidebar.selectbox("í”„ë¡œì„¸ìŠ¤ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš” ğŸ”„", df.columns)
        bytes_col = st.sidebar.selectbox("ë°”ì´íŠ¸ ì „ì†¡ëŸ‰ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš” ğŸ“Š", df.columns)

        st.sidebar.markdown("### ğŸ¯ ëª¨ë¸ íŒŒë¼ë¯¸í„°")
        lr = st.sidebar.number_input("Learning Rate âœ¨", value=0.0005, format="%.4f")
        hidden_dim = st.sidebar.number_input("Hidden Dimension ğŸˆ", value=128, min_value=32, max_value=512)
        time_window = st.sidebar.number_input("Time Window â±ï¸", value=7200, min_value=1800)
        threshold = st.sidebar.number_input("Threshold ğŸ“Š", value=8.0, format="%.1f")

        # ê³ ê¸‰ ì„¤ì •
        with st.sidebar.expander("ğŸ”§ ê³ ê¸‰ ì„¤ì •"):
            warm_up_period = st.number_input("Warm-up Period", value=1000, min_value=100)
            update_interval = st.number_input("Update Interval", value=200, min_value=50)
            max_history_length = st.number_input("Max History Length", value=2000, min_value=500)
            checkpoint_interval = st.number_input("Checkpoint Interval", value=10000, min_value=1000)

        # ë©”ì¸ ì˜ì—­ì— ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
        st.markdown("### ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df.head())

        # ì‹¤í–‰ ë²„íŠ¼

        if st.sidebar.button("âœ¨ ì´ìƒì¹˜ íƒì§€ ì‹œì‘!", help="í´ë¦­í•˜ë©´ ì´ìƒì¹˜ íƒì§€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤"):
            try:
                with st.spinner("ğŸ” ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘..."):
                    # ì›ë³¸ ë°ì´í„° ì €ì¥
                    original_df = df.copy()

                    # ì „ì²˜ë¦¬
                    df_processed, user_mapping, proc_mapping, user_encoder, proc_encoder, _ = preprocess_edr_data(
                        df=df,
                        user_col=user_col,
                        proc_col=proc_col,
                        time_col=time_col,
                        bytes_col=bytes_col
                    )

                # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
                params = {
                    'lr': lr,
                    'hidden_dim': hidden_dim,
                    'time_window': time_window,
                    'threshold': threshold,
                    'warm_up_period': warm_up_period,
                    'update_interval': update_interval,
                    'max_history_length': max_history_length,
                    'checkpoint_interval': checkpoint_interval,
                    'user_col': user_col,
                    'proc_col': proc_col,
                    'time_col': time_col,
                    'bytes_col': bytes_col
                }

                # ëª¨ë¸ í•™ìŠµ ë° ì´ìƒì¹˜ íƒì§€ - ì›ë³¸ ë°ì´í„° ì „ë‹¬ ì¶”ê°€
                model, results_df, history, losses = edr_online_train_and_detect(
                    df_processed,
                    len(user_mapping),
                    len(proc_mapping),
                    user_encoder,
                    proc_encoder,
                    original_df,
                    **params
                )

                # ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
                analyze_and_visualize_results(results_df, losses)

                # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                st.download_button(
                    label="ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                    data=results_df.to_csv(index=False),
                    file_name="anomaly_detection_results.csv",
                    mime="text/csv"
                )

            except Exception as e:
                st.error(f"ğŸ˜¢ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

    else:
        # íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œ í‘œì‹œí•  ë©”ì‹œì§€
        st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”! ğŸ™‚")

        # ì‚¬ìš© ë°©ë²• ì•ˆë‚´
        st.markdown("""
        ### ğŸ“Œ ì‚¬ìš© ë°©ë²•
        1. ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”
        2. í•„ìš”í•œ ì»¬ëŸ¼ë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”
        3. ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”
        4. 'ì´ìƒì¹˜ íƒì§€ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”

        ### ğŸ¯ ì£¼ìš” ê¸°ëŠ¥
        - IP ê¸°ë°˜ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„
        - ì‹¤ì‹œê°„ ì´ìƒì¹˜ íƒì§€
        - ê²°ê³¼ ì‹œê°í™” ë° ë‹¤ìš´ë¡œë“œ
        """)


if __name__ == "__main__":
    main()
